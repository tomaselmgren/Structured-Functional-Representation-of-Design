{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d5ee47-f77d-4d50-94c4-a58f7cf00330",
   "metadata": {},
   "source": [
    "# Structured Functional Representation of Design\n",
    "\n",
    "This short code snippet uses a non-recursive method to extract Functional Requirements (FRs) and their corresponding Design Parameters (DPs), up to a depth=3 (this can easily be expanded by repeating the calls more times in a loop), and with a adjustable number of nodes per level (see the function get_top_n_items()).\n",
    "\n",
    "To run the model, an OpenAI API key is required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76497127-420a-49c9-a988-e68565e8c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "def get_top_n_items(text: str, n: int = 3):\n",
    "    \"\"\"Extracts the top `n` items from a bullet-pointed list of responses.\"\"\"\n",
    "    items = re.findall(r'\\n-\\s*(.*)', text) \n",
    "    return items[:n] if items else []\n",
    "\n",
    "class StructuredDesign:\n",
    "    def __init__(self, dev_prompt: str, content_prompt: str, q0: str, qfr: str, qdp: str):\n",
    "\n",
    "        # Regular API / Model Settings\n",
    "        self.model = \"gpt-4o-2024-08-06\"\n",
    "        self.max_generation_tokens: int = 500\n",
    "        self.temperature: float = 0.0\n",
    "\n",
    "        # Don't forget to use your own API key here\n",
    "        self.client = OpenAI(api_key=\"\")\n",
    "\n",
    "        # Prompts \n",
    "        self.dev_prompt = dev_prompt\n",
    "        self.content_prompt = content_prompt\n",
    "        self.q0 = q0\n",
    "        self.qfr = qfr\n",
    "        self.qdp = qdp\n",
    "\n",
    "        # Arrays for storing outputs\n",
    "        self.fr_dp_pairs = [] \n",
    "        self.dp_fr_pairs = []\n",
    "\n",
    "    def _llm_api_call(self, context: str, query: str):\n",
    "        \"\"\" Reusable function to make an API call to the OpenAI API. \"\"\"\n",
    "        dev_content = self.dev_prompt.format(context=context)\n",
    "        user_content = self.content_prompt.format(query=query)\n",
    "    \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"developer\", \"content\": dev_content},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "            ],\n",
    "            \"max_tokens\": self.max_generation_tokens,\n",
    "            \"temperature\": self.temperature,\n",
    "        }\n",
    "    \n",
    "        res = self.client.chat.completions.create(**payload).choices[0].message.content\n",
    "        return res\n",
    "\n",
    "    def generate_top_frs(self, context: str, n: int = 3):\n",
    "        \"\"\" Generate the top `n` Functional Requirements (FRs). \"\"\"\n",
    "        response = self._llm_api_call(context=context, query=self.q0)\n",
    "        return get_top_n_items(response, n)\n",
    "\n",
    "    def generate_top_dps(self, fr_list: list, context: str, n: int = 3):\n",
    "        \"\"\" Generate the top `n` Design Parameters (DPs) for each FR. \"\"\"\n",
    "        for fr in fr_list:\n",
    "            dp_response = self._llm_api_call(context=context, query=self.qdp.format(fr=fr))\n",
    "            top_dps = get_top_n_items(dp_response, n)\n",
    "            for dp in top_dps:\n",
    "                self.fr_dp_pairs.append((fr, dp))\n",
    "        return self.fr_dp_pairs\n",
    "\n",
    "    def extract_top_frs_for_dps(self, dp_list: list, context: str, n: int = 3):\n",
    "        \"\"\" Extract the top `n` FRs for each DP.\"\"\"\n",
    "        for dp in dp_list:\n",
    "            fr_response = self._llm_api_call(context=context, query=self.qdp.format(fr=dp))\n",
    "            top_frs = get_top_n_items(fr_response, n)\n",
    "            for fr in top_frs:\n",
    "                self.dp_fr_pairs.append((dp, fr))\n",
    "        return self.dp_fr_pairs\n",
    "\n",
    "sd = StructuredDesign(\n",
    "    dev_prompt = \"You are extracting functional requirements from engineering documents: '{context}'. Provide a bullet-point list.\", \n",
    "    content_prompt = \"'{query}'\",\n",
    "    q0 = \"What is the aim of this document? List all functional requirements in bullet points.\",\n",
    "    qfr = \"What is required for '{dp_sup}' to achieve '{fr_sup}'?\",\n",
    "    qdp = \"How does this achieve '{fr}'?\"\n",
    ")\n",
    "\n",
    "document_text = r\"\"\"Your text here\"\"\"\n",
    "\n",
    "# Step 1: Generate Top FRs\n",
    "frs = sd.generate_top_frs(document_text, n=3)\n",
    "print(\"Top Functional Requirements:\")\n",
    "print(frs)\n",
    "\n",
    "# Step 2: Generate Top DPs for Each FR\n",
    "dp_pairs = sd.generate_top_dps(frs, document_text, n=3)\n",
    "print(\"\\nTop Design Parameters for Selected FRs:\")\n",
    "for fr, dp in dp_pairs:\n",
    "    print(f\"FR: {fr} -> DP: {dp}\")\n",
    "\n",
    "# Step 3: Extract FRs for DPs\n",
    "dps = [dp for _, dp in dp_pairs]\n",
    "dp_fr_pairs = sd.extract_top_frs_for_dps(dps, document_text, n=3)\n",
    "print(\"\\nExtracted FRs from DPs:\")\n",
    "for dp, fr in dp_fr_pairs:\n",
    "    print(f\"DP: {dp} -> FR: {fr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c30391-be98-46ba-89cb-fb698c1e8e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
